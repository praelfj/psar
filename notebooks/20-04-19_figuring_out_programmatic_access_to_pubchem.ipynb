{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic Access to PubChem Troubleshooting\n",
    "\n",
    "__goal__: figure out how to download tabular bioassay data from pubchem bioassay based on queries and UID lists\n",
    "\n",
    "__approach__: go through the PubChem docs to find out how to do this in the most efficient way possible\n",
    "\n",
    "---\n",
    "## Programmatic Access\n",
    "https://pubchemdocs.ncbi.nlm.nih.gov/programmatic-access\n",
    "* there are multiple points of programmatic access to PubChem:\n",
    "    1. __PUG-REST__ - a simplified, URL-based access system to PubChem data that is designed for locating specific data from specific pubchem recrods. PUG-REST is designed such that it has less overhead than the XML associated wit PUG and the SOAP envelopes that PUG-SOAP use, so it is easier and quicker to use, but lacks the added capabilities that the other web services can accomplish. It is made to handle short requests (<30 seconds) that are synchronous (happening during the requesting period). _Given the simple task you need to accomplish, this is likely your best bet_\n",
    "        * _Representational state transfer (REST)_ - a set of rules for interacting with web services that adherese to specific style of software architecture. \n",
    "    * __PUG-View__ - a REST-based platform that returns summaries of records; used mainly for creating PubChem summary web pages\n",
    "    * __Power User Gateway (PUG)__ - uses a common gateway interface (CGI) called <code>pug.cgi</code> to exchange XML data through HTTP POSTs\n",
    "    * __PUG-SOAP__ - Simple Object Access Protocol (SOAP)-based access service. Easier programmatic access than the PUG gateway, with enhanced flexibility of data access relative to REST protocols, but still has added complexity in interpretation because of the SOAP envelopes it returns. Recommended for GUI workflow applications like pipeline pilot and for programming/scripting langauges like Python\n",
    "    * __PuChemRDF REST interface__ - REST itnerface for RDF data\n",
    "    * __Entrez Utilities__ - E-utilites; not well suited to PubChem because it cannot easily access/return a lot of data types unique to the PubChem system, such as large tables of bioactivity data, or chemical structures \n",
    "* these were made because the chemical and bioassay data within pubchem is structured differently than the data in the rest of Entrez, so to make things easier, they created a new set of interfaces with PubChem data: the Power User Gateway (PUG)\n",
    "\n",
    "---\n",
    "## PUG-SOAP\n",
    "https://pubchemdocs.ncbi.nlm.nih.gov/pug-soap\n",
    "* this looks like it's meant to handle more complex queries and information retreival, so I'm going to shelve learning this for now\n",
    "* just note that it is an option, if you find yourself needing to access information in a more complicated way across the database\n",
    "\n",
    "---\n",
    "## PUG-REST\n",
    "https://pubchemdocs.ncbi.nlm.nih.gov/pug-rest-tutorial\n",
    "### introduction\n",
    "* developed to be a simple interface for scripts, javascript applications in webpages, or other apps, to pull data from pubchem\n",
    "* it is designed to pull specific types of information, as opposed to summary statments across records in the database\n",
    "\n",
    "__usage notes__:\n",
    "* it is not intended for millions of requests - smaller batches are prefered\n",
    "* time limit for an access request: access requests should not be too complicated (i.e. cannot go over 30 seconds per request)\n",
    "* request volume limitations: the amount of requests per second are limited (do not exceed 5 req/sec); dynamic request throttling can be used to help you maintain under the limit\\\n",
    "* not adhering to these rules could get your IP address temporarily banned from using any PUG service\n",
    "\n",
    "### how PUG REST works\n",
    "The PUG REST workflow (i.e. URL) has 3x required parts that vary:\n",
    "1. an input - how data within the database should be recognized (e.g a compound to search by as a SMILES string or a AID record)\n",
    "2. an operation - how you want the data to be processed (e.g. returing tabular bioactivity data)\n",
    "3. an output - how you want the data returned (e.g. as a CSV file)\n",
    "These portions of the request are modular, so they can be swapped around with different requests to yield different pieces of information based on the data\n",
    "\n",
    "For example, to look up the InChI for Vioxx as text, the URL would be this:\n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/vioxx/property/InChI/TXT\n",
    "\n",
    "When you go to the above link, PUG REST takes the HTTP/HTTPS request, parses it, figures out the input (Vioxx compund name) does the requested operation encoded into the URL path (look up the InChI key), and returns it as specified (as text)\n",
    "\n",
    "4. additionally, a <code>?</code> following the link an be used to add additionally information for certain queries\n",
    "\n",
    "### design of the URL\n",
    "In the case of Vioxx ambove:\n",
    "> __https://pubchem.ncbi.nlm.nih.gov/rest/pug__ | represents the invariant portion of the link, or prolog<br>\n",
    "__/compound/name/vioxx__ | is the input (look up all compound names that match the query \"vioxx\")<br>\n",
    "__/property/InChI__ | is the operation (look up the InChI key)<br>\n",
    "__/TXT__ | is the output (return as text)<br>\n",
    "\n",
    "if the request has chracters that cannot be interpreted as a URL, or is too long, then you can use HTTP POST to get around these shortcomings\n",
    "\n",
    "### output\n",
    "types of output formats:\n",
    "\n",
    "| output format | description |\n",
    "| --- | --- |\n",
    "| XML | standard XML, for which a schema is available |\n",
    "| JSON | JavaScript Object Notation |\n",
    "| JSONP | like JSON, but wrapped in a callback function |\n",
    "| ASNB | standard binary ASN.1 |\n",
    "| ASNT | NCBI's human-readable text flavor of ASN.1 |\n",
    "| SDF | chemical structure data format |\n",
    "| CSV | comma separated values |\n",
    "| PNG | PNG image |\n",
    "| TXT | plain text |\n",
    "\n",
    "note that these are specific for each record that you are retrieving - (cannot return an SDF for a data table, for example)\n",
    "\n",
    "### error handling\n",
    "when an error occurs during a request, PUG-REST returns a human readable error message as to what happened, in lieu of the requested data\n",
    "\n",
    "### Access to PubChem BioAssays\n",
    "there are two main ways to access assay data on PubChem: via AID or via SID/CID\n",
    "\n",
    "__accessing data via AID__\n",
    "using a specific AID, you can access a variety of information surrounding the assay, and the assay data itself, in a straightforward manner:\n",
    "* assay description - title, protocol, etc. associated with the assay\n",
    "> e.g. https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/504526/description/XML\n",
    "* target of the assay - biological data surrounding the target of the assay\n",
    "> e.g. the gene symbol, protein name, and other terms for a target of a given assay<br>\n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/490,1000/targets/ProteinGI,ProteinName,GeneID,GeneSymbol/XML\n",
    "* subsets of assay data or small assay data sets - note that by default, you can't go over 10,000 rows/request, so you cannot download large datasets in this way\n",
    "\n",
    "> e.g. downloading a small data set: <br>\n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/504526/CSV\n",
    "\n",
    ">e.g. downloading a subset of SIDs from a dataset:<br> \n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/504526/XML?sid=104169547,109967232\n",
    "\n",
    ">e.g. downloading dose-response data in a simplified output:<br> \n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/504526/doseresponse/CSV?sid=104169547,109967232\n",
    "\n",
    "* entire assay data sets - see the \"dealing with lists of identifiers\" section below to learn how to request very large datasets from PubChem BioAssays\n",
    "\n",
    "__retrieving AIDs that match a query__\n",
    "* you can search by different assay types and get the AIDs returned via including aids in the operation\n",
    "* furthermore, you can specify by assay type using the <code>activity</code> operator\n",
    "> e.g. finding all assays that are measuring EC50<br>\n",
    "https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/activity/EC50/aids/JSON\n",
    "\n",
    "__accessing data via CID/SID__\n",
    "* you can find all the assay information for a given CID/SID as follows:\n",
    ">e.g. https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/1000,1001/assaysummary/CSV\n",
    "* this base URL can be modified to pull actual assay data, or other information surrounding how the compound was discovered\n",
    "\n",
    "### Dealing with lists of identifiers\n",
    "__storing lists on the server__\n",
    "* this is useful for very long lists of identifiers, e.g. when you're downloading thousands of SIDs from a given AID to get around the 10,000 line cap that one request has (if you try and request a dataset of >10,000 records, PubChem REST will return an error)\n",
    "* alternatively, this is useful for when you have one set of IDs that you want to use on many requests\n",
    "* for these examples, you can store the lists server-side, then retrieve them in batches to do operations on them\n",
    "* a __list key__ is a key you use to access your server-side data\n",
    "* you use the list key to repeatedly access the data and handle it in batches\n",
    "\n",
    "__case study: downloading all of the SIDs in a large assay__:\n",
    "first, request the list of SIDs associated with the assay, and store them as a listkey\n",
    ">https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/640/sids/XML?list_return=listkey\n",
    "\n",
    "the above link contains, in XML format, the list key itself, and also descriptions of the data set - like the number of records found in it. The list key is stored under the ListKey leaf, and the # of records is stored under the Size leaf\n",
    "\n",
    "next, use the list key to download the data in batches from PUG-REST. Loop over the following data using the URL below, but updating the <code>listkey_start=</code> to iterate over the entire data set\n",
    "> https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/640/CSV?sid=listkey&listkey=2452489925683562238&listkey_start=0&listkey_count=1000\n",
    "\n",
    "after you reach the full size of the data set, you're done!\n",
    "\n",
    "---\n",
    "\n",
    "## testing download of a single dataset\n",
    "AID = 640\n",
    "number of records = 96409\n",
    "link to downloading the whole dataset from the GUI: https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?query=download&record_type=datatable&actvty=all&response_type=save&aid=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a listkey\n",
    "aid = '640'\n",
    "url = 'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{}/sids/XML?list_return=listkey'.format(aid)\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "xml = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out the links within the attribute values of the IdentifierList tag to make ET happy\n",
    "cleaned_xml = xml.replace('\\n', '')\n",
    "\n",
    "id_start = cleaned_xml.find('<IdentifierList')\n",
    "listkey_start = cleaned_xml.find('ListKey')\n",
    "cleaned_xml = cleaned_xml[0:id_start] + '<IdentifierList>' + cleaned_xml[listkey_start-1:]\n",
    "cleaned_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out the list key and size from the xml\n",
    "root = ET.fromstring(cleaned_xml)\n",
    "listkey = root.find('ListKey').text\n",
    "size = root.find('Size').text\n",
    "\n",
    "print(listkey, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the keys and construct a dataframe in batches\n",
    "url_prolog = 'https://pubchem.ncbi.nlm.nih.gov/rest/pug/'\n",
    "url_listkey = 'assay/aid/{aid}/CSV?sid=listkey&listkey={listkey}'.format(aid=aid,\n",
    "                                                                        listkey=listkey)\n",
    "url_base = url_prolog + url_listkey\n",
    "\n",
    "listkey_start = 0\n",
    "listkey_count = 1000\n",
    "listkey_end = listkey_start + listkey_count\n",
    "row_count = 0\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "while listkey_end < 10000:\n",
    "    url_keycount = '&listkey_start={listkey_start}&listkey_count={listkey_count}'.format(listkey_start=listkey_start,\n",
    "                                                                                         listkey_count = listkey_count)\n",
    "    dl_url = url_base + url_keycount\n",
    "    \n",
    "    # read in the data as DF and get rid of the 3x useless rows at the top: result_type, result_descr, result_unit\n",
    "    temp_df = pd.read_csv(dl_url, index_col=0)\n",
    "    data = pd.concat([data, temp_df], sort=False)\n",
    "    \n",
    "    if row_count%10000 == 0:\n",
    "        print('{row} rows of {size} rows completed'.format(row=row_count, size=size))\n",
    "    \n",
    "    listkey_start += listkey_count\n",
    "    listkey_end += listkey_count\n",
    "    row_count += listkey_count\n",
    "    \n",
    "# save the dataframe\n",
    "p = '../data/test_download/{}_test.csv'.format(aid)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code does not work right\n",
    "downloading it that way leaves out certain rows - we have 96288 rows in lieu of the 96409 we were expecting \n",
    "\n",
    "furthermore, the garbage columns:\n",
    ">0,RESULT_TYPE,,,,,,,FLOAT<br>\n",
    "1,RESULT_DESCR,,,,,,,Normalized % inhibition at 2 micromolar inhibitor concentration of the primary assay<br>\n",
    "2,RESULT_UNIT,,,,,,,NONE\n",
    "\n",
    "are repeated for each call to pubchem - meaning we're actually missing even MORE rows, because there are 97 * 3 rows of that nonsense include within the above data set\n",
    "\n",
    "---\n",
    "\n",
    "## conclusions\n",
    "* given that the data seems to be missing lines and is loading extraordinarly slowly, the GUI interface is probably better for batch-downloading full datasets, regardless of what the documentation says\n",
    "* I should use the GUI interface for downloading bioassay data\n",
    "* I can, however, use this interface if I have more complicated queries across certain compounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qsar]",
   "language": "python",
   "name": "conda-env-qsar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
