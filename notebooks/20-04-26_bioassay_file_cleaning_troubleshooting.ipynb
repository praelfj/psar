{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# troubleshooting pubchem bioassay cleaning\n",
    "__rationale__: I am trying to reproduce Helal, Kazi Yasin, et al. JCIM 56.2 (2016): 390-398. I've downloaded the pubchem records they used in their analysis, and now I have to make sure the data are suitable to be analyzed by cleaning the associated files.\n",
    "\n",
    "__notes on pubchem recrods__:\n",
    "* first 4x rows lack activity data - are annotations for the data contained in the file\n",
    "* I should dump all useless records - internal IDs for each institution, blank rows, etc. - I need to narrow down in on the bare minimum variables I need to get the analysis done\n",
    "* CIDs need to be converted from float to int if that's not done already in the file\n",
    "* some of them have replicates, which will need to be averaged later on - probably on the processing step\n",
    "* activity data has a unique column name on each record - yeesh - how do I located this in records? Also, some screens were done with multiple replicates, so in those files, the data will have to be averaged together ...\n",
    "* PUBCHEM_ACTIVITY_SCORE is a user-defined metric between 100 and 0 that allows users to prioritize hits - don't use this for your z-score calculation, because it seems to be extremely subjective and therefore variable between records\n",
    "* the same compounds are screened multiple times in one screen - therefore, when calculating z scores later on, I need to account for \n",
    "\n",
    "__ideas on how to find activity data__\n",
    "* can't do certain float - stdev, z-scores, etc. would come down\n",
    "* can't do name, because the name varies with each and every assay - no consistencies between records\n",
    "* can't do result description, because while some are the name of the active column, some are multi-line descriptions of the data\n",
    "* can't do the XML metadata, because there's no explicit \"active column\" tag, just the description data that's in the flat CSV file\n",
    "* ... maybe I can do the column after the PUBCHEM_ASSAYDATA_COMMENT - so far, all of the assays I've checked have had either (a) the activity data or (b) the first replicate appear after this column\n",
    "\n",
    "# pick up here - need to figure out how to parse out the activity data\n",
    "# I think \n",
    "# that the column after the comment tag above might be the consistent organization in the data -> np.where, then i + 1\n",
    "\n",
    "__approach__:\n",
    "* manually look through select files over the course of the time period to see if the formatting is consistent between files\n",
    "* remove the first rows which lack information on compound activity\n",
    "* ensure that the data type for \n",
    "\n",
    "---\n",
    "## troubleshooting CID extraction and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, skip desc rows\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid1 = pd.read_csv(p + '2130.csv', skiprows=[1,2,3,4])\n",
    "aid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract CIDs, convert to ints, count\n",
    "aid1['PUBCHEM_CID'].astype(int).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm ... how should we deal with the fact that some compounds are screened more than once?\n",
    "\n",
    "Right now, having unique counts isn't the point ... we don't want to know the absolute number of times a compound has been screened - we want to know how many assays the compound has been screened in. Therfore, this information can be dropped here\n",
    "\n",
    "... however, this will have to be accounted for later on - during the Z-score step, I'm going to have to average values for a given record. I'm noting this at the top of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dict = {'PUBCHEM_CID':aid1['PUBCHEM_CID'].astype(int).unique(),\n",
    "              'current_count':1}\n",
    "intermediate_CID = pd.DataFrame(inter_dict)\n",
    "intermediate_CID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start df to hold data over loops\n",
    "cids = pd.DataFrame(columns=['PUBCHEM_CID', 'assay_count'])\n",
    "\n",
    "# merge old and new\n",
    "cids = pd.merge(cids, intermediate_CID, on='PUBCHEM_CID', how='outer')\n",
    "\n",
    "# fill NaN with 0 to allow proper addition\n",
    "cids = cids.fillna(0)\n",
    "\n",
    "# add and drop the current count column\n",
    "cids['assay_count'] = cids['assay_count'] + cids['current_count']\n",
    "cids = cids.drop('current_count', axis=1)\n",
    "cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to merge second column by hand:\n",
    "\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid1 = pd.read_csv(p + '435005.csv', skiprows=[1,2,3,4])\n",
    "\n",
    "aid1['PUBCHEM_CID'].astype(int).value_counts()\n",
    "\n",
    "inter_dict = {'PUBCHEM_CID':aid1['PUBCHEM_CID'].astype(int).unique(),\n",
    "              'current_count':1}\n",
    "intermediate_CID = pd.DataFrame(inter_dict)\n",
    "\n",
    "\n",
    "cids = pd.merge(cids, intermediate_CID, on='PUBCHEM_CID', how='outer')\n",
    "\n",
    "cids = cids.fillna(0)\n",
    "\n",
    "cids['assay_count'] = cids['assay_count'] + cids['current_count']\n",
    "cids.drop('current_count', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that should do the trick!\n",
    "\n",
    "let's write the loop and run a pilot on a couple AIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports and initialize dataframe to hold data\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid_list = ['1511.csv', '2029.csv', '493036.csv', '602162.csv', '720543.csv']\n",
    "\n",
    "cids = pd.DataFrame(columns=['PUBCHEM_CID', 'assay_count'])\n",
    "aid_count = 1\n",
    "\n",
    "# loop over data to import, process, and add to the cids DF\n",
    "for f in aid_list:\n",
    "    # import and skip over desc rows\n",
    "    aid = pd.read_csv(p + f, skiprows=[1,2,3,4])\n",
    "    \n",
    "    # correct CID data type, isolate unique CIDs\n",
    "    inter_dict = {'PUBCHEM_CID':aid['PUBCHEM_CID'].astype(int).unique(),\n",
    "              'current_count':1}\n",
    "    intermediate_CID = pd.DataFrame(inter_dict)\n",
    "    \n",
    "    # merge with cids DF\n",
    "    cids = pd.merge(cids, intermediate_CID, on='PUBCHEM_CID', how='outer')\n",
    "\n",
    "    # add records and clean merged df\n",
    "    cids = cids.fillna(0)\n",
    "    cids['assay_count'] = cids['assay_count'] + cids['current_count']\n",
    "    cids = cids.drop('current_count', axis=1)\n",
    "    \n",
    "    print('{count} of {total} AIDs processed'.format(count=aid_count,\n",
    "                                                     total=len(aid_list)))\n",
    "    aid_count += 1\n",
    "\n",
    "cids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " yikes - missing values in the initial DFs ... let's see what's giving us the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, skip desc rows\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid1 = pd.read_csv(p + '602162.csv', skiprows=[1,2,3,4])\n",
    "aid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid1['PUBCHEM_CID'].loc[aid1['PUBCHEM_CID'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid1.loc[348664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid1['PUBCHEM_CID'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whoops - looks just like there's an issue here - let's get rid of all the records like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports and initialize dataframe to hold data\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid_list = ['1511.csv', '2029.csv', '493036.csv', '602162.csv', '720543.csv']\n",
    "\n",
    "cids = pd.DataFrame(columns=['PUBCHEM_CID', 'assay_count'])\n",
    "aid_count = 1\n",
    "\n",
    "# loop over data to import, process, and add to the cids DF\n",
    "for f in aid_list:\n",
    "    # import and skip over desc rows\n",
    "    aid = pd.read_csv(p + f, skiprows=[1,2,3])\n",
    "    \n",
    "    # correct CID data type, isolate unique CIDs\n",
    "    cid_series = aid['PUBCHEM_CID'].dropna()\n",
    "    inter_dict = {'PUBCHEM_CID':cid_series.astype(int).unique(),\n",
    "              'current_count':1}\n",
    "    intermediate_CID = pd.DataFrame(inter_dict)\n",
    "    \n",
    "    # merge with cids DF\n",
    "    cids = pd.merge(cids, intermediate_CID, on='PUBCHEM_CID', how='outer')\n",
    "\n",
    "    # add records and clean merged df\n",
    "    cids = cids.fillna(0)\n",
    "    cids['assay_count'] = cids['assay_count'] + cids['current_count']\n",
    "    cids = cids.drop('current_count', axis=1)\n",
    "    \n",
    "    print('{count} of {total} AIDs processed'.format(count=aid_count,\n",
    "                                                     total=len(aid_list)))\n",
    "    aid_count += 1\n",
    "\n",
    "cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on a different data set to manually curate\n",
    "# set up imports and initialize dataframe to hold data\n",
    "p = '../data/1-raw/public-domain-fingerprints/'\n",
    "aid_list = ['1511.csv', '1554.csv', '1662.csv', '1663.csv', '1813.csv']\n",
    "\n",
    "cids = pd.DataFrame(columns=['PUBCHEM_CID', 'assay_count'])\n",
    "aid_count = 1\n",
    "\n",
    "# loop over data to import, process, and add to the cids DF\n",
    "for f in aid_list:\n",
    "    # import and skip over desc rows\n",
    "    aid = pd.read_csv(p + f, skiprows=[1,2,3])\n",
    "    \n",
    "    # correct CID data type, isolate unique CIDs\n",
    "    cid_series = aid['PUBCHEM_CID'].dropna()\n",
    "    inter_dict = {'PUBCHEM_CID':cid_series.astype(int).unique(),\n",
    "              'current_count':1}\n",
    "    intermediate_CID = pd.DataFrame(inter_dict)\n",
    "    \n",
    "    # merge with cids DF\n",
    "    cids = pd.merge(cids, intermediate_CID, on='PUBCHEM_CID', how='outer')\n",
    "\n",
    "    # add records and clean merged df\n",
    "    cids = cids.fillna(0)\n",
    "    cids['assay_count'] = cids['assay_count'] + cids['current_count']\n",
    "    cids = cids.drop('current_count', axis=1)\n",
    "    \n",
    "    print('{count} of {total} AIDs processed'.format(count=aid_count,\n",
    "                                                     total=len(aid_list)))\n",
    "    aid_count += 1\n",
    "\n",
    "cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these check out by hand!\n",
    "\n",
    "It's remarkable that the screening sets that went into each of these initial assays are so similar, but based on the ones I tested, the compounds coming up in 5/5 or 4/5 all came up in their predicted counts when I went in and searched these files by hand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qsar]",
   "language": "python",
   "name": "conda-env-qsar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
